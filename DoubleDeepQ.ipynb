{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 23:08:27.562623: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-01 23:08:27.563070: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 23:08:27.564737: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from dict import WORD_BANK\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "keys = list(WORD_BANK.keys()) # shorten list for validation purposed\n",
    "random.shuffle(keys)\n",
    "keys = keys[:500]\n",
    "WORD_BANK = {key: tf.constant(i, dtype = tf.int32) for i, key in enumerate(keys)}\n",
    "INDEX_TO_WORD = [key for key in keys]\n",
    "\n",
    "VOCAB_SIZE = len(WORD_BANK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "LETTER_TO_INDEX = {i: letter for letter, i in enumerate(ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(keras.Model):\n",
    "    def __init__(self, dropout_rate = .2, reg = None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        A deep Q net for wordle \n",
    "\n",
    "        state: \n",
    "            grey: 1, 26, locality does not matter\n",
    "            yellow: word_len, 26, locality does matter\n",
    "            green: word_len, 26, locality does matter\n",
    "            \n",
    "            (N, 11, 26)\n",
    "\n",
    "        action:\n",
    "            (N, VOCAB_SIZE) choice a letter bases on state \n",
    "        \"\"\"\n",
    "\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "\n",
    "        self.dense0 = keras.layers.Dense(512, activation = 'relu', kernel_regularizer = reg) # 26 inputs for each color; grey, yellow, green\n",
    "        self.dense1 = keras.layers.Dense(1028, activation = 'relu', kernel_regularizer = reg)\n",
    "        self.dense2 = keras.layers.Dense(2024, activation = 'relu', kernel_regularizer = reg)\n",
    "        self.dense3 = keras.layers.Dense(VOCAB_SIZE, activation  = 'softmax')\n",
    "\n",
    "        self.dropout0 = keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.dropout = bool(dropout_rate)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.flatten(input)\n",
    "        if self.dropout:\n",
    "            x = self.dense0(x)\n",
    "            x = self.dropout0(x)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.dropout2(x)\n",
    "        else:\n",
    "            x = self.dense0(x)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "\n",
    "        output = self.dense3(x)\n",
    "        return output\n",
    "\n",
    "class DoubleDeepQ():\n",
    "    def __init__(self, \n",
    "        loss = tf.keras.losses.huber,\n",
    "        opt = tf.keras.optimizers.Adam(0.0001),\n",
    "        gamma = .,5 \n",
    "        reg = tf.keras.regularizers.l2(),\n",
    "        dropout_rate = 0\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Q = Qnet(reg = reg, dropout_rate = dropout_rate)\n",
    "        self.Q_target = Qnet()\n",
    "\n",
    "        self.transfer_weights()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.opt = opt\n",
    "        self.loss = loss\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.Q_target(state)\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        bellman eq: Q(s, a) = Q(s, a) + alpha * (r + gamma * max(Q(s', a) - Q(s, a)))\n",
    "\n",
    "        state: tensor (N, 15, 27) \n",
    "        action: tensor int (N, 1) with axis 1: index\n",
    "        reward: tensor (N, 1)\n",
    "        \"\"\"\n",
    "        mask = tf.one_hot(action, VOCAB_SIZE) # action is a one hot vector\n",
    "\n",
    "        future_reward = self.Q_target(next_state)\n",
    "        updated_q = reward + self.gamma * tf.reduce_max(future_reward) # update q value\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_value = self.Q(state) # get q values of each action in the state\n",
    "            q_of_action = tf.reduce_sum(q_value * mask) # 0 out other action values and sum to get \n",
    "            loss = self.loss(updated_q, q_of_action) # calc loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.Q.trainable_variables) \n",
    "        self.opt.apply_gradients(zip(grads, self.Q.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def best_action(self, state):\n",
    "        return tf.argmax(self.Q_target(state), axis = 1)\n",
    "        \n",
    "    def transfer_weights(self):\n",
    "        self.Q_target.set_weights(self.Q.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, max_size = 10_000):\n",
    "        self.max_size = max_size\n",
    "        self.list = ([], [], [], [])\n",
    "\n",
    "    def append(self, state, action, reward, next_state):\n",
    "        if len(self.list) >= self.max_size - 1:\n",
    "            pop_index = random.randint(0, self.max_size - 2)\n",
    "            for i in range(4): self.list[i].pop(pop_index)\n",
    "    \n",
    "        for i, item in enumerate((state, action, reward, next_state)):\n",
    "            self.list[i].append(item)\n",
    "\n",
    "    def unpack_random(self, batch_size):\n",
    "        length = len(self)\n",
    "        indices = random.sample(range(length), batch_size)\n",
    "\n",
    "        state = list(map(lambda i: self.list[0][i], indices))\n",
    "        action = list(map(lambda i: self.list[1][i], indices))\n",
    "        reward = list(map(lambda i: self.list[2][i], indices))\n",
    "        next_state = list(map(lambda i: self.list[3][i], indices))\n",
    "\n",
    "        state = tf.stack(state)\n",
    "        action = tf.stack(action)\n",
    "        reward = tf.stack(reward)\n",
    "        next_state = tf.stack(next_state)\n",
    "\n",
    "        return state, action, reward, next_state\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions provide nice flexible abstrations, they make refactoring much easier \n",
    "\n",
    "def action_to_word(action): \n",
    "    \"\"\"action: a (5, 1) value\"\"\"\n",
    "    return INDEX_TO_WORD[int(action)]\n",
    "\n",
    "def word_to_action(word):\n",
    "    action = WORD_BANK[word]\n",
    "    return tf.constant(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_lose = tf.constant(0, dtype = tf.float32)\n",
    "reward_win = tf.constant(20, dtype = tf.float32)\n",
    "reward_valid = tf.constant(0, dtype = tf.float32) # valid word reward \n",
    "reward_green = tf.constant(4, dtype = tf.float32) # green letter reward\n",
    "reward_yellow = tf.constant(2, dtype = tf.float32) # yellow letter reward\n",
    "\n",
    "games_per_train = 20# how many games to train on \n",
    "batch_per_epoch = 30\n",
    "games_per_test = 20\n",
    "\n",
    "epsilon = .3\n",
    "\n",
    "model = DoubleDeepQ()\n",
    "\n",
    "green_offset = 0\n",
    "yellow_offset = 5\n",
    "grey_offset = 10\n",
    "\n",
    "buffer = Buffer()\n",
    "batch_size = 64\n",
    "\n",
    "STATE_SHAPE = (11, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  159, Loss: 1.93725, Avg reward: 4.08933, epsilon: 0.300:  53%|█████▎    | 159/300 [1:06:48<59:15, 25.21s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4411/990116660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0min_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcorrect_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreen_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLETTER_TO_INDEX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "random_action = lambda : word_to_action(random.choice(list(WORD_BANK)))\n",
    "loss_avg = 0\n",
    "reward_avg = 0\n",
    "\n",
    "exp_avg = lambda avg, r: avg * .99 + r * .01\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(f\"Epoch: {epoch:4d}, Loss: {loss_avg:.5f}, Avg reward: {reward_avg:.5f}, epsilon: {epsilon:.3f}\")\n",
    "    for _ in range(batch_per_epoch):\n",
    "        for game in range(games_per_train):\n",
    "            correct_word = random.choice(list(WORD_BANK)) # randomly select correct work\n",
    "            state = tf.zeros((1, *STATE_SHAPE)) # init state\n",
    "            row = 0 # init the row that the game is on\n",
    "            while row < 20:\n",
    "                actions = model(state)\n",
    "                action = tf.argmax(actions, axis = 1, output_type = tf.int32)[0] if random.random() > epsilon else random_action()\n",
    "\n",
    "                word = action_to_word(action)\n",
    "\n",
    "                if word == correct_word: # of the corrct word is guessed\n",
    "                    reward = reward_win\n",
    "                    reward_avg = exp_avg(reward_avg, reward)\n",
    "                    buffer.append(state, action, reward, state)\n",
    "                    break # break if correct word was guessed \n",
    "\n",
    "                reward = tf.constant(0, dtype = tf.float32)\n",
    "                \n",
    "                indices = [] # indices where words match in correct possition\n",
    "                # check to see if letters match in word in the correct position\n",
    "                for i in range(5):\n",
    "                    in_word = False\n",
    "                    for j in range(5):\n",
    "                        if word[i] == correct_word[j]:\n",
    "                            if i == j:\n",
    "                                indices.append([0, green_offset + i, LETTER_TO_INDEX[word[i]]])\n",
    "                                in_word = True\n",
    "                                reward += reward_green\n",
    "                                break\n",
    "                            else:\n",
    "                                indices.append([0, yellow_offset + i, LETTER_TO_INDEX[word[i]]])\n",
    "                                in_word = True\n",
    "                                reward += reward_yellow\n",
    "                                break\n",
    "                    if not in_word:\n",
    "                        indices.append([0, grey_offset, LETTER_TO_INDEX[word[i]]])\n",
    "\n",
    "                values = [1] * len(indices) # create a list of ones the same length as indices \n",
    "                next_state = tf.tensor_scatter_nd_update(state, indices, values) # update new state \n",
    "\n",
    "                buffer.append(state, action, reward, next_state)\n",
    "                state = next_state # set state to next state\n",
    "\n",
    "                reward_avg = exp_avg(reward_avg, reward)\n",
    "\n",
    "                row += 1\n",
    "\n",
    "        # train model \n",
    "        if len(buffer) >= batch_size:\n",
    "            state, action, reward, next_state = buffer.unpack_random(batch_size = batch_size)\n",
    "            loss = model.update_model(state, action, reward, next_state)\n",
    "            loss_avg = loss_avg * 0.995 + loss * 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=901>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae84cfcbf12a373d25f2dc3fc1524df64200ac869aa1bffef005b4f2595896b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf2_p38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
